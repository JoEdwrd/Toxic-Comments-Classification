{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"},{"sourceId":11650,"sourceType":"datasetVersion","datasetId":8327},{"sourceId":19053,"sourceType":"datasetVersion","datasetId":14154}],"dockerImageVersionId":25160,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q watermark\n!pip install --ignore-installed PyYAML\n!pip install transformers\n%load_ext watermark\n%watermark -p torch,pandas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip cache purge","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade transformers huggingface_hub\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install virtualenv\n!virtualenv myenv\n!source myenv/bin/activate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade pip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoTokenizer\nimport torch.nn as nn\nimport os\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, Dense, LSTM, GRU, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, add, SpatialDropout1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\n\n# Hyperparameters\nMAX_LEN = 250\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = TRAIN_BATCH_SIZE * 2\nEPOCHS = 5\nLEARNING_RATE = 1e-05\nEMBEDDING_FILE = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nDEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(\"Device:\", DEVICE)\n\n# Load data\ntrain = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n\n# Use AutoTokenizer from Hugging Face\nhf_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Tokenize data\ntrain_encodings = hf_tokenizer(train[\"comment_text\"].fillna(\"fillna\").tolist(), truncation=True, padding=True, max_length=MAX_LEN)\ntest_encodings = hf_tokenizer(test[\"comment_text\"].fillna(\"fillna\").tolist(), truncation=True, padding=True, max_length=MAX_LEN)\n\nx_train = np.array(train_encodings['input_ids'])\nx_test = np.array(test_encodings['input_ids'])\n\ny_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n\n# Load FastText embeddings\ndef get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\n\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n\nword_index = hf_tokenizer.get_vocab()\nmax_features = min(150000, len(word_index) + 1)\nembed_size = 300\nembedding_matrix = np.zeros((max_features, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\ndef build_model(embedding_matrix):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(LSTM(512, return_sequences=True))(x)\n    x = Bidirectional(GRU(512, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(2048, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(2048, activation='relu')(hidden)])\n    result = Dense(6, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=result)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return model\n\n\ntokenizer = text.Tokenizer(filters='')\ntokenizer.fit_on_texts(list(train[\"comment_text\"].fillna(\"fillna\").tolist()) + list(test[\"comment_text\"].fillna(\"fillna\").tolist()))\n\nx_train = tokenizer.texts_to_sequences(train[\"comment_text\"].fillna(\"fillna\").tolist())\nx_test = tokenizer.texts_to_sequences(test[\"comment_text\"].fillna(\"fillna\").tolist())\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(x_train, y_train, test_size=0.1)\n\n# Training loop\nSEEDS = 10\npred = 0\n\nfor seed in range(SEEDS):\n    model = build_model(embedding_matrix)\n    for epoch in range(EPOCHS):\n        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n\n        history = model.fit(\n                    X_train,\n                    Y_train,\n                    validation_data=(X_valid, Y_valid),\n                    batch_size=128,\n                    epochs=1,\n                    verbose=2,\n                    callbacks=[\n                        LearningRateScheduler(lambda _: 1e-3 * (0.5 ** epoch))\n                    ]\n                )\n\n        train_loss = history.history['loss'][0]\n        train_accuracy = history.history['acc'][0]\n        val_loss = history.history['val_loss'][0]\n        val_accuracy = history.history['val_acc'][0]\n\n        print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\n        train_preds = model.predict(X_train)\n        train_auc = np.mean([roc_auc_score(Y_train[:, i], train_preds[:, i]) for i in range(Y_train.shape[1])])\n        print(f\"Training ROC AUC: {train_auc:.4f}\")\n\n        val_preds = model.predict(X_valid)\n        val_preds_binary = (val_preds > 0.5).astype(int)\n\n        # Calculate F1 scores, avoiding issues with no positive predictions\n        f1_scores = []\n        for i in range(Y_valid.shape[1]):\n            if np.sum(val_preds_binary[:, i]) == 0:\n                f1_scores.append(0.0)\n            else:\n                f1_scores.append(f1_score(Y_valid[:, i], val_preds_binary[:, i], zero_division=1))\n        avg_f1_score = np.mean(f1_scores)\n\n        print(f\"Validation F1 Score: {avg_f1_score:.4f}\")\n\n        AUC = np.mean([roc_auc_score(Y_valid[:, i], val_preds[:, i]) for i in range(Y_valid.shape[1])])\n        print(f\"Validation ROC AUC: {AUC:.4f}\")\n\n    pred += model.predict(x_test, batch_size=1024, verbose=1) / SEEDS\n    np.save('pred', pred)\n\n    model.save_weights(f'model_weights_{seed}.h5')\n    os.system(f'gzip model_weights_{seed}.h5')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}