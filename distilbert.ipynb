{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"},{"sourceId":19053,"sourceType":"datasetVersion","datasetId":14154}],"dockerImageVersionId":30262,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoTokenizer\nimport torch.nn as nn\nimport os\n\n# Hyperparameters\nMAX_LEN = 250\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = TRAIN_BATCH_SIZE * 2\nEPOCHS = 5\nLEARNING_RATE = 1e-05\nEMBEDDING_FILE = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nDEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(\"Device:\", DEVICE)\n\n# Load data\ntrain = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n\n# Use AutoTokenizer from Hugging Face\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Tokenize data\ntrain_encodings = tokenizer(train[\"comment_text\"].fillna(\"fillna\").tolist(), truncation=True, padding=True, max_length=MAX_LEN)\ntest_encodings = tokenizer(test[\"comment_text\"].fillna(\"fillna\").tolist(), truncation=True, padding=True, max_length=MAX_LEN)\n\nx_train = np.array(train_encodings['input_ids'])\nx_test = np.array(test_encodings['input_ids'])\n\ny_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n\n# Load FastText embeddings\ndef get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n\nword_index = tokenizer.get_vocab()\nmax_features = min(150000, len(word_index) + 1)\nembed_size = 300\nembedding_matrix = np.zeros((max_features, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\nclass MultiLabelDataset(Dataset):\n    def __init__(self, inputs, targets=None):\n        self.inputs = inputs\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, index):\n        input_ids = torch.tensor(self.inputs[index], dtype=torch.long)\n        if self.targets is not None:\n            targets = torch.tensor(self.targets[index], dtype=torch.float)\n            return input_ids, targets\n        return input_ids\n\ntrain_size = 0.8\ntrain_inputs, val_inputs, train_targets, val_targets = train_test_split(x_train, y_train, train_size=train_size, random_state=42)\n\ntrain_dataset = MultiLabelDataset(train_inputs, train_targets)\nval_dataset = MultiLabelDataset(val_inputs, val_targets)\ntest_dataset = MultiLabelDataset(x_test)  # Create test dataset\n\ntrain_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=2)  # Create test DataLoader\n\nclass TransformerModel(nn.Module):\n    def __init__(self, max_features, embed_size, embedding_matrix):\n        super(TransformerModel, self).__init__()\n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n\n        self.bert = AutoModel.from_pretrained('distilbert-base-uncased')\n        self.classifier = nn.Sequential(\n            nn.Linear(768 + embed_size, 768),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(768, 6)\n        )\n\n    def forward(self, input_ids, fasttext_embeddings):\n        attention_mask = (input_ids > 0).long()\n        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        bert_cls = bert_outputs.last_hidden_state[:, 0, :]\n\n        # Apply mean pooling to fasttext_embeddings\n        fasttext_embeddings = fasttext_embeddings.mean(dim=1)\n\n        combined = torch.cat((bert_cls, fasttext_embeddings), 1)\n        logits = self.classifier(combined)\n        return logits\n\nmodel = TransformerModel(max_features, embed_size, embedding_matrix)\nmodel.to(DEVICE)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\nloss_func = nn.BCEWithLogitsLoss()\nlr_sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n\ndef train_one_epoch(train_loader, model, loss_func, optimizer):\n    model.train()\n    size = len(train_loader.dataset)\n    all_targets = []\n    all_outputs = []\n    total_loss = 0\n    \n    for i, (input_ids, targets) in enumerate(train_loader):\n        input_ids = input_ids.to(DEVICE)\n        targets = targets.to(DEVICE)\n        \n        # Get FastText embeddings\n        fasttext_embeddings = model.embedding(input_ids)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, fasttext_embeddings)\n        loss = loss_func(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        all_targets.append(targets.cpu().numpy())\n        all_outputs.append(outputs.sigmoid().detach().cpu().numpy())\n\n        if i % 100 == 0:\n            loss_val = loss.item()\n            current = i * len(input_ids)\n            print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]\")\n    \n    avg_loss = total_loss / len(train_loader)\n    all_targets = np.concatenate(all_targets)\n    all_outputs = np.concatenate(all_outputs)\n    train_acc = accuracy_score(all_targets, all_outputs.round())\n    train_roc_auc = roc_auc_score(all_targets, all_outputs, average='macro')\n    train_f1 = f1_score(all_targets, all_outputs.round(), average='macro')\n\n    print(f\"Training Error: \\n Avg loss: {avg_loss:>8f} \\n Accuracy: {train_acc:>8f} \\n ROC AUC: {train_roc_auc:>8f} \\n F1 Score: {train_f1:>8f} \\n\")\n    return avg_loss, train_acc, train_roc_auc, train_f1\n\ndef validate_one_epoch(val_loader, model, loss_func):\n    model.eval()\n    size = len(val_loader.dataset)\n    num_batches = len(val_loader)\n    val_loss = 0\n    all_targets = []\n    all_outputs = []\n    with torch.no_grad():\n        for input_ids, targets in val_loader:\n            input_ids = input_ids.to(DEVICE)\n            targets = targets.to(DEVICE)\n            \n            # Get FastText embeddings\n            fasttext_embeddings = model.embedding(input_ids)\n\n            outputs = model(input_ids, fasttext_embeddings)\n            val_loss += loss_func(outputs, targets).item()\n            \n            all_targets.append(targets.cpu().numpy())\n            all_outputs.append(outputs.sigmoid().cpu().numpy())\n\n    val_loss /= num_batches\n    all_targets = np.concatenate(all_targets)\n    all_outputs = np.concatenate(all_outputs)\n    val_acc = accuracy_score(all_targets, all_outputs.round())\n    val_roc_auc = roc_auc_score(all_targets, all_outputs, average='macro')\n    val_f1 = f1_score(all_targets, all_outputs.round(), average='macro')\n\n    print(f\"Validation Error: \\n Avg loss: {val_loss:>8f} \\n Accuracy: {val_acc:>8f} \\n ROC AUC: {val_roc_auc:>8f} \\n F1 Score: {val_f1:>8f} \\n\")\n    return val_loss, val_acc, val_roc_auc, val_f1\n\ndef evaluate(loader, model):\n    model.eval()\n    all_outputs = []\n    with torch.no_grad():\n        for input_ids in loader:\n            input_ids = input_ids.to(DEVICE)\n            \n            # Get FastText embeddings\n            fasttext_embeddings = model.embedding(input_ids)\n\n            outputs = model(input_ids, fasttext_embeddings)\n            all_outputs.append(outputs.sigmoid().cpu().numpy())\n\n    all_outputs = np.concatenate(all_outputs)\n    return all_outputs\n\n# Training loop\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1} (lr = {lr_sched.get_last_lr()[0]:.2e})\\n-------------------------------\")\n    train_loss, train_acc, train_roc_auc, train_f1 = train_one_epoch(train_loader, model, loss_func, optimizer)\n    val_loss, val_acc, val_roc_auc, val_f1 = validate_one_epoch(val_loader, model, loss_func)\n    lr_sched.step()\n\n# Evaluate on test set\ntest_outputs = evaluate(test_loader, model)\n# Assuming the test labels are available in y_test\n# y_test = ... # Load the test labels similarly to y_train\n\n# If y_test is available, calculate the metrics\n# test_acc = accuracy_score(y_test, test_outputs.round())\n# test_roc_auc = roc_auc_score(y_test, test_outputs, average='macro')\n# test_f1 = f1_score(y_test, test_outputs.round(), average='macro')\n\n# print(f\"Test Accuracy: {test_acc:>8f} \\n Test ROC AUC: {test_roc_auc:>8f} \\n Test F1 Score: {test_f1:>8f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}